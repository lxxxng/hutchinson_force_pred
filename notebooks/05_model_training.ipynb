{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e882268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_data = joblib.load('../data/interim/05_X_new_features_scaled.joblib')\n",
    "y_data = joblib.load('../data/interim/06_y_combined.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "478b3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356224, 91)\n",
      "(356224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60b0c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356224 entries, 0 to 356223\n",
      "Data columns (total 91 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Time                   356224 non-null  float64\n",
      " 1   1_red                  356224 non-null  float64\n",
      " 2   1_blue                 356224 non-null  float64\n",
      " 3   1_yellow               356224 non-null  float64\n",
      " 4   2_red                  356224 non-null  float64\n",
      " 5   2_blue                 356224 non-null  float64\n",
      " 6   2_yellow               356224 non-null  float64\n",
      " 7   3_red                  356224 non-null  float64\n",
      " 8   3_blue                 356224 non-null  float64\n",
      " 9   3_yellow               356224 non-null  float64\n",
      " 10  4_red                  356224 non-null  float64\n",
      " 11  4_blue                 356224 non-null  float64\n",
      " 12  4_yellow               356224 non-null  float64\n",
      " 13  5_red                  356224 non-null  float64\n",
      " 14  5_blue                 356224 non-null  float64\n",
      " 15  5_yellow               356224 non-null  float64\n",
      " 16  6_red                  356224 non-null  float64\n",
      " 17  6_blue                 356224 non-null  float64\n",
      " 18  6_yellow               356224 non-null  float64\n",
      " 19  global_mean            356224 non-null  float64\n",
      " 20  global_std             356224 non-null  float64\n",
      " 21  global_sum             356224 non-null  float64\n",
      " 22  red_color_mean         356224 non-null  float64\n",
      " 23  red_color_std          356224 non-null  float64\n",
      " 24  blue_color_mean        356224 non-null  float64\n",
      " 25  blue_color_std         356224 non-null  float64\n",
      " 26  yellow_color_mean      356224 non-null  float64\n",
      " 27  yellow_color_std       356224 non-null  float64\n",
      " 28  1_red_rolling_mean     356224 non-null  float64\n",
      " 29  1_red_rolling_std      356224 non-null  float64\n",
      " 30  1_blue_rolling_mean    356224 non-null  float64\n",
      " 31  1_blue_rolling_std     356224 non-null  float64\n",
      " 32  1_yellow_rolling_mean  356224 non-null  float64\n",
      " 33  1_yellow_rolling_std   356224 non-null  float64\n",
      " 34  2_red_rolling_mean     356224 non-null  float64\n",
      " 35  2_red_rolling_std      356224 non-null  float64\n",
      " 36  2_blue_rolling_mean    356224 non-null  float64\n",
      " 37  2_blue_rolling_std     356224 non-null  float64\n",
      " 38  2_yellow_rolling_mean  356224 non-null  float64\n",
      " 39  2_yellow_rolling_std   356224 non-null  float64\n",
      " 40  3_red_rolling_mean     356224 non-null  float64\n",
      " 41  3_red_rolling_std      356224 non-null  float64\n",
      " 42  3_blue_rolling_mean    356224 non-null  float64\n",
      " 43  3_blue_rolling_std     356224 non-null  float64\n",
      " 44  3_yellow_rolling_mean  356224 non-null  float64\n",
      " 45  3_yellow_rolling_std   356224 non-null  float64\n",
      " 46  4_red_rolling_mean     356224 non-null  float64\n",
      " 47  4_red_rolling_std      356224 non-null  float64\n",
      " 48  4_blue_rolling_mean    356224 non-null  float64\n",
      " 49  4_blue_rolling_std     356224 non-null  float64\n",
      " 50  4_yellow_rolling_mean  356224 non-null  float64\n",
      " 51  4_yellow_rolling_std   356224 non-null  float64\n",
      " 52  5_red_rolling_mean     356224 non-null  float64\n",
      " 53  5_red_rolling_std      356224 non-null  float64\n",
      " 54  5_blue_rolling_mean    356224 non-null  float64\n",
      " 55  5_blue_rolling_std     356224 non-null  float64\n",
      " 56  5_yellow_rolling_mean  356224 non-null  float64\n",
      " 57  5_yellow_rolling_std   356224 non-null  float64\n",
      " 58  6_red_rolling_mean     356224 non-null  float64\n",
      " 59  6_red_rolling_std      356224 non-null  float64\n",
      " 60  6_blue_rolling_mean    356224 non-null  float64\n",
      " 61  6_blue_rolling_std     356224 non-null  float64\n",
      " 62  6_yellow_rolling_mean  356224 non-null  float64\n",
      " 63  6_yellow_rolling_std   356224 non-null  float64\n",
      " 64  1_red_diff             356224 non-null  float64\n",
      " 65  1_blue_diff            356224 non-null  float64\n",
      " 66  1_yellow_diff          356224 non-null  float64\n",
      " 67  2_red_diff             356224 non-null  float64\n",
      " 68  2_blue_diff            356224 non-null  float64\n",
      " 69  2_yellow_diff          356224 non-null  float64\n",
      " 70  3_red_diff             356224 non-null  float64\n",
      " 71  3_blue_diff            356224 non-null  float64\n",
      " 72  3_yellow_diff          356224 non-null  float64\n",
      " 73  4_red_diff             356224 non-null  float64\n",
      " 74  4_blue_diff            356224 non-null  float64\n",
      " 75  4_yellow_diff          356224 non-null  float64\n",
      " 76  5_red_diff             356224 non-null  float64\n",
      " 77  5_blue_diff            356224 non-null  float64\n",
      " 78  5_yellow_diff          356224 non-null  float64\n",
      " 79  6_red_diff             356224 non-null  float64\n",
      " 80  6_blue_diff            356224 non-null  float64\n",
      " 81  6_yellow_diff          356224 non-null  float64\n",
      " 82  block_1_3_mean         356224 non-null  float64\n",
      " 83  block_4_6_mean         356224 non-null  float64\n",
      " 84  block_1_3_std          356224 non-null  float64\n",
      " 85  block_4_6_std          356224 non-null  float64\n",
      " 86  PCA_1                  356224 non-null  float64\n",
      " 87  PCA_2                  356224 non-null  float64\n",
      " 88  PCA_3                  356224 non-null  float64\n",
      " 89  PCA_4                  356224 non-null  float64\n",
      " 90  PCA_5                  356224 non-null  float64\n",
      "dtypes: float64(91)\n",
      "memory usage: 247.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c7297cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(X_data, threshold=0.9, verbose=True):\n",
    "    \"\"\"\n",
    "    Returns names of features with reduced correlation (|corr| <= threshold).\n",
    "\n",
    "    Parameters:\n",
    "    - X_data (pd.DataFrame): The input DataFrame.\n",
    "    - threshold (float): Correlation threshold to consider for removal.\n",
    "    - verbose (bool): If True, prints the pairs and dropped columns.\n",
    "\n",
    "    Returns:\n",
    "    - remaining_features (list): List of remaining feature names.\n",
    "    - dropped_features (list): List of dropped feature names.\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = X_data.corr().abs()\n",
    "\n",
    "    # Mask the diagonal and lower triangle\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find highly correlated pairs\n",
    "    high_corr_pairs = [\n",
    "        (col1, col2, corr_value)\n",
    "        for col1 in upper.columns\n",
    "        for col2, corr_value in upper[col1].items()\n",
    "        if corr_value > threshold\n",
    "    ]\n",
    "\n",
    "    # Optionally print the pairs\n",
    "    if verbose:\n",
    "        if high_corr_pairs:\n",
    "            print(\"Highly correlated feature pairs:\")\n",
    "            for col1, col2, corr_value in high_corr_pairs:\n",
    "                print(f\"{col1} and {col2} : {corr_value:.2f}\")\n",
    "        else:\n",
    "            print(\"No feature pairs exceed the correlation threshold.\")\n",
    "\n",
    "    # Identify columns to drop (second column of each pair)\n",
    "    to_drop = {col2 for col1, col2, corr_value in high_corr_pairs}\n",
    "\n",
    "    # Get remaining features\n",
    "    remaining_features = [col for col in X_data.columns if col not in to_drop]\n",
    "\n",
    "    # Optionally print the dropped and remaining columns\n",
    "    if verbose:\n",
    "        print(f\"Dropped columns: {list(to_drop)}\")\n",
    "        print(f\"Remaining columns: {len(remaining_features)} features\")\n",
    "\n",
    "    return remaining_features, list(to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c8c61",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b9cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Features: 19\n",
      "PCA Features: 5\n",
      "Color Features: 6\n",
      "Rolling Features: 36\n",
      "Temporal Features: 18\n",
      "Sensor Features: 4\n"
     ]
    }
   ],
   "source": [
    "original_features = X_data.iloc[:, 0:19].columns.tolist()\n",
    "pca_features = [i for i in X_data.columns if \"PCA\" in i]\n",
    "color_features = [i for i in X_data.columns if \"color\" in i]\n",
    "rolling_features = [i for i in X_data.columns if \"rolling\" in i]\n",
    "temporal_features = [i for i in X_data.columns if \"diff\" in i]\n",
    "sensor_features = [i for i in X_data.columns if \"block\" in i]\n",
    "\n",
    "print(\"Original Features:\", len(original_features))\n",
    "print(\"PCA Features:\", len(pca_features))\n",
    "print(\"Color Features:\", len(color_features))\n",
    "print(\"Rolling Features:\", len(rolling_features))\n",
    "print(\"Temporal Features:\", len(temporal_features))\n",
    "print(\"Sensor Features:\", len(sensor_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a22d11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_1 = original_features\n",
    "feature_set_2 = pca_features\n",
    "feature_set_3 = original_features + rolling_features + temporal_features \n",
    "feature_set_4 = original_features + color_features + sensor_features\n",
    "feature_set_5 = original_features + pca_features + color_features + rolling_features + temporal_features + sensor_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b23fc",
   "metadata": {},
   "source": [
    "### Remove features that are highly corr with one another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16fa3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_6, dropped_columns = remove_highly_correlated_features(X_data, threshold=0.8, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed89daa",
   "metadata": {},
   "source": [
    "### Feature Importance Ranking + Top-K Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef64dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_features_with_importance(X_data, y_data, top_k=10, model=None, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Train a RandomForestRegressor and select top-k important features.\n",
    "\n",
    "    Parameters:\n",
    "    - X_data (pd.DataFrame): Feature matrix\n",
    "    - y_data (pd.Series or pd.DataFrame): Target values\n",
    "    - top_k (int): Number of top features to select\n",
    "    - model (sklearn model, optional): Provide a pre-configured model if desired\n",
    "    - n_jobs (int): Number of parallel jobs (-1 uses all cores)\n",
    "\n",
    "    Returns:\n",
    "    - top_features (list): List of top-k feature names\n",
    "    - feature_importance_ranking (pd.DataFrame): Full ranking of features and importances\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor(n_jobs=n_jobs) \n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(X_data, y_data)\n",
    "\n",
    "    # Get importances\n",
    "    importances = model.feature_importances_\n",
    "    feature_ranks = sorted(zip(X_data.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Prepare outputs\n",
    "    top_features = [f[0] for f in feature_ranks[:top_k]]\n",
    "    feature_importance_ranking = pd.DataFrame(feature_ranks, columns=['Feature', 'Importance'])\n",
    "\n",
    "    return top_features, feature_importance_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79a7c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_7, feature_importance_ranking = select_top_k_features_with_importance(X_data, y_data, top_k=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce07e3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Features: ['1_blue_rolling_mean', '6_blue', '1_blue', '2_yellow_rolling_std', '1_red_rolling_mean', '1_yellow', 'global_std', '3_yellow', '3_red_rolling_std', '6_yellow']\n",
      "                Feature  Importance\n",
      "0   1_blue_rolling_mean    0.403088\n",
      "1                6_blue    0.234097\n",
      "2                1_blue    0.099106\n",
      "3  2_yellow_rolling_std    0.073315\n",
      "4    1_red_rolling_mean    0.057883\n",
      "5              1_yellow    0.026352\n",
      "6            global_std    0.011763\n",
      "7              3_yellow    0.007368\n",
      "8     3_red_rolling_std    0.006304\n",
      "9              6_yellow    0.005401\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 10 Features:\", feature_set_7)\n",
    "print(feature_importance_ranking.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2f75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results from above\n",
    "# feature_set_7 = ['1_blue_rolling_mean', \n",
    "#                  '6_blue', \n",
    "#                  '1_blue', \n",
    "#                  '2_yellow_rolling_std', \n",
    "#                  '1_red_rolling_mean', \n",
    "#                  '1_yellow', \n",
    "#                  'global_std', \n",
    "#                  '3_yellow', \n",
    "#                  '3_red_rolling_std', \n",
    "#                  '6_yellow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fff56a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature_set_1:  19\n",
      "length of feature_set_2:  5\n",
      "length of feature_set_3:  73\n",
      "length of feature_set_4:  29\n",
      "length of feature_set_5:  88\n",
      "length of feature_set_6:  33\n",
      "length of feature_set_7:  10\n"
     ]
    }
   ],
   "source": [
    "feature_subset_list = [\n",
    "    feature_set_1, \n",
    "    feature_set_2, \n",
    "    feature_set_3, \n",
    "    feature_set_4, \n",
    "    feature_set_5, \n",
    "    feature_set_6, \n",
    "    feature_set_7\n",
    "]\n",
    "\n",
    "for i, subset in enumerate(feature_subset_list):\n",
    "    print(f\"length of feature_set_{i + 1}: \", len(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9df0b0",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6dfc87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "098fe964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets,\n",
    "    model_class,\n",
    "    model_name,\n",
    "    model_save_dir='../model/',\n",
    "    multi_output=False,\n",
    "    **model_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic function to evaluate multiple feature subsets with optional multi-output support,\n",
    "    save the best model, and report performance.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train, X_val, y_val: Pre-split data\n",
    "    - feature_subsets (list): List of feature name lists\n",
    "    - model_class: Scikit-learn model class (e.g., RandomForestRegressor)\n",
    "    - model_name (str): Name to tag the saved model file\n",
    "    - model_save_dir (str): Directory to save model\n",
    "    - multi_output (bool): Whether to wrap with MultiOutputRegressor\n",
    "    - model_kwargs: Additional model parameters\n",
    "\n",
    "    Returns:\n",
    "    - best_model: Trained best model\n",
    "    - best_metrics (dict): R2, MAE, RMSE\n",
    "    - best_features (list): Features used\n",
    "    \"\"\"\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_metrics = {}\n",
    "    best_features = []\n",
    "\n",
    "    for idx, feature_subset in enumerate(feature_subsets, start=1):\n",
    "        print(f\"\\nEvaluating Feature Set {idx} with {len(feature_subset)} features using {model_name}...\")\n",
    "\n",
    "        X_train_subset = X_train[feature_subset]\n",
    "        X_val_subset = X_val[feature_subset]\n",
    "\n",
    "        # Build model with optional multi-output wrapper\n",
    "        base_model = model_class(**model_kwargs)\n",
    "        model = MultiOutputRegressor(base_model) if multi_output else base_model\n",
    "        \n",
    "        print(f\"Full model parameters:\\n{base_model.get_params()}\\n\")\n",
    "\n",
    "        # Train and predict\n",
    "        model.fit(X_train_subset, y_train)\n",
    "        y_pred = model.predict(X_val_subset)\n",
    "         \n",
    "        # Check and print iteration count if available for mlp\n",
    "        if hasattr(model, 'n_iter_'):\n",
    "            print(f\"Model stopped after {model.n_iter_} iterations.\")\n",
    "        elif hasattr(model, 'estimators_') and hasattr(model.estimators_[0], 'n_iter_'):\n",
    "            print(f\"Model stopped after {model.estimators_[0].n_iter_} iterations (first output).\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "        print(f\"R² Score: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "        # Update best model if better\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_metrics = {'R2': r2, 'MAE': mae, 'RMSE': rmse}\n",
    "            best_features = feature_subset\n",
    "\n",
    "    # Save best model\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_save_dir, f'best_{model_name}.joblib')\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"\\n✅ Best {model_name} model saved to: {model_path}\")\n",
    "\n",
    "    return best_model, best_metrics, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb2e1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Feature Set 1 with 19 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9754 | MAE: 0.0526 | RMSE: 0.1187\n",
      "\n",
      "Evaluating Feature Set 2 with 5 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9255 | MAE: 0.1037 | RMSE: 0.2018\n",
      "\n",
      "Evaluating Feature Set 3 with 73 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9936 | MAE: 0.0233 | RMSE: 0.0584\n",
      "\n",
      "Evaluating Feature Set 4 with 29 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9748 | MAE: 0.0536 | RMSE: 0.1200\n",
      "\n",
      "Evaluating Feature Set 5 with 88 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9932 | MAE: 0.0241 | RMSE: 0.0603\n",
      "\n",
      "Evaluating Feature Set 6 with 33 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9786 | MAE: 0.0454 | RMSE: 0.1017\n",
      "\n",
      "Evaluating Feature Set 7 with 10 features using random_forest...\n",
      "Full model parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9902 | MAE: 0.0312 | RMSE: 0.0758\n",
      "\n",
      "✅ Best random_forest model saved to: ../model/best_random_forest.joblib\n"
     ]
    }
   ],
   "source": [
    "# Random Forest \n",
    "rf_model_2, rf_metrics_2, rf_features_2 = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=RandomForestRegressor,\n",
    "    model_name='random_forest',\n",
    "    random_state=42,\n",
    "    n_jobs=-1  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e7bc83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Feature Set 1 with 19 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9675 | MAE: 0.0708 | RMSE: 0.1364\n",
      "\n",
      "Evaluating Feature Set 2 with 5 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9217 | MAE: 0.1094 | RMSE: 0.2066\n",
      "\n",
      "Evaluating Feature Set 3 with 73 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9873 | MAE: 0.0482 | RMSE: 0.0840\n",
      "\n",
      "Evaluating Feature Set 4 with 29 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9677 | MAE: 0.0710 | RMSE: 0.1363\n",
      "\n",
      "Evaluating Feature Set 5 with 88 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9874 | MAE: 0.0481 | RMSE: 0.0835\n",
      "\n",
      "Evaluating Feature Set 6 with 33 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9729 | MAE: 0.0690 | RMSE: 0.1245\n",
      "\n",
      "Evaluating Feature Set 7 with 10 features using hist_gradient_boosting...\n",
      "Full model parameters:\n",
      "{'categorical_features': None, 'early_stopping': 'auto', 'interaction_cst': None, 'l2_regularization': 0.0, 'learning_rate': 0.1, 'loss': 'squared_error', 'max_bins': 255, 'max_depth': None, 'max_iter': 500, 'max_leaf_nodes': 31, 'min_samples_leaf': 20, 'monotonic_cst': None, 'n_iter_no_change': 10, 'quantile': None, 'random_state': 42, 'scoring': 'loss', 'tol': 1e-07, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9802 | MAE: 0.0581 | RMSE: 0.1054\n",
      "\n",
      "✅ Best hist_gradient_boosting model saved to: ../model/best_hist_gradient_boosting.joblib\n"
     ]
    }
   ],
   "source": [
    "# HistGradientBoostingRegressor\n",
    "gb_model_wrapped, gb_metrics, gb_features = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=HistGradientBoostingRegressor,\n",
    "    model_name='hist_gradient_boosting',\n",
    "    multi_output=True,  \n",
    "    random_state=42,\n",
    "    max_iter=500,\n",
    "    learning_rate=0.1,\n",
    "    max_leaf_nodes=31,\n",
    "    min_samples_leaf=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cb7c07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Feature Set 1 with 19 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9627 | MAE: 0.0760 | RMSE: 0.1431\n",
      "\n",
      "Evaluating Feature Set 2 with 5 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9262 | MAE: 0.1079 | RMSE: 0.2010\n",
      "\n",
      "Evaluating Feature Set 3 with 73 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9835 | MAE: 0.0500 | RMSE: 0.0822\n",
      "\n",
      "Evaluating Feature Set 4 with 29 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9624 | MAE: 0.0755 | RMSE: 0.1430\n",
      "\n",
      "Evaluating Feature Set 5 with 88 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9823 | MAE: 0.0502 | RMSE: 0.0867\n",
      "\n",
      "Evaluating Feature Set 6 with 33 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9695 | MAE: 0.0683 | RMSE: 0.1219\n",
      "\n",
      "Evaluating Feature Set 7 with 10 features using mlp...\n",
      "Full model parameters:\n",
      "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': True, 'epsilon': 1e-08, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 1000, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': 42, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n",
      "\n",
      "R² Score: 0.9654 | MAE: 0.0733 | RMSE: 0.1361\n",
      "\n",
      "✅ Best mlp model saved to: ../model/best_mlp.joblib\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "mlp_model, mlp_metrics, mlp_features = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=MLPRegressor,\n",
    "    model_name='mlp',\n",
    "    random_state=42,\n",
    "    max_iter=1000,                \n",
    "    hidden_layer_sizes=(100, 50), \n",
    "    activation='relu',           \n",
    "    solver='adam',               \n",
    "    learning_rate='adaptive',     \n",
    "    early_stopping=True           \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca6e9e",
   "metadata": {},
   "source": [
    "# Grid search for feature_set_7 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21031642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_and_report_single_subset(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_set,\n",
    "    param_grid,\n",
    "    model_class,\n",
    "    model_name,\n",
    "    model_save_dir='../model/',\n",
    "    multi_output=False,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5\n",
    "):\n",
    "    \"\"\"\n",
    "    GridSearchCV for a single feature set with optional multi-output support.\n",
    "    Reports R2, MAE, RMSE on validation set and saves the best model.\n",
    "\n",
    "    Returns:\n",
    "    - best_model\n",
    "    - best_params\n",
    "    - validation_metrics\n",
    "    \"\"\"\n",
    "\n",
    "    X_train_subset = X_train[feature_set]\n",
    "    X_val_subset = X_val[feature_set]\n",
    "\n",
    "    # Handle wrapping if needed\n",
    "    base_model = model_class()\n",
    "    model_to_search = MultiOutputRegressor(base_model) if multi_output else base_model\n",
    "\n",
    "    # Prepare GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model_to_search,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        verbose=2,\n",
    "        scoring=scoring\n",
    "    )\n",
    "\n",
    "    # Run Grid Search\n",
    "    grid_search.fit(X_train_subset, y_train)\n",
    "\n",
    "    # Get Best Model and Params\n",
    "    best_model = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"\\n✅ Best Parameters Found: {best_params}\")\n",
    "\n",
    "    # Evaluate on Validation Set\n",
    "    y_pred = best_model.predict(X_val_subset)\n",
    "    r2 = r2_score(y_val, y_pred)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "    print(f\"✅ Validation R²: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # Check and print iteration count if available\n",
    "    if hasattr(best_model, 'n_iter_'):\n",
    "        print(f\"Model stopped after {best_model.n_iter_} iterations.\")\n",
    "    elif hasattr(best_model, 'estimators_') and hasattr(best_model.estimators_[0], 'n_iter_'):\n",
    "        print(f\"Model stopped after {best_model.estimators_[0].n_iter_} iterations (first output).\")\n",
    "\n",
    "    # Save Model\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_save_dir, f'10_features_{model_name}_gridsearch.joblib')\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"✅ Best {model_name} model saved to: {model_path}\")\n",
    "\n",
    "    validation_metrics = {'R2': r2, 'MAE': mae, 'RMSE': rmse}\n",
    "\n",
    "    return best_model, best_params, validation_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539ac70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "source": [
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt'],\n",
    "    'max_samples': [0.5, 0.75, None],\n",
    "}\n",
    "\n",
    "best_model_rf, best_params_rf, val_metrics_rf = grid_search_and_report_single_subset(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_set_7,\n",
    "    param_grid_rf,\n",
    "    model_class=RandomForestRegressor,\n",
    "    model_name='random_forest',\n",
    "    multi_output=False  # RF handles multi-output natively\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "166067cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
      "\n",
      "✅ Best Parameters Found: {'estimator__l2_regularization': 0.01, 'estimator__learning_rate': 0.1, 'estimator__max_iter': 700, 'estimator__max_leaf_nodes': 127, 'estimator__min_samples_leaf': 10}\n",
      "✅ Validation R²: 0.9921 | MAE: 0.0354 | RMSE: 0.0650\n",
      "Model stopped after 700 iterations (first output).\n",
      "✅ Best hist_gradient_boosting model saved to: ../model/10_features_hist_gradient_boosting_gridsearch.joblib\n"
     ]
    }
   ],
   "source": [
    "param_grid_hist = {\n",
    "    'estimator__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'estimator__max_iter': [300, 500, 700],\n",
    "    'estimator__max_leaf_nodes': [31, 63, 127],\n",
    "    'estimator__min_samples_leaf': [10, 20, 50],\n",
    "    'estimator__l2_regularization': [0.0, 0.01, 0.1]\n",
    "}\n",
    "\n",
    "best_model_hist, best_params_hist, val_metrics_hist = grid_search_and_report_single_subset(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_set_7,\n",
    "    param_grid_hist,\n",
    "    model_class=HistGradientBoostingRegressor,\n",
    "    model_name='hist_gradient_boosting',\n",
    "    multi_output=True  # HistGB needs MultiOutputRegressor\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1524b1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 144 candidates, totalling 720 fits\n",
      "\n",
      "✅ Best Parameters Found: {'activation': 'relu', 'alpha': 0.0001, 'early_stopping': True, 'hidden_layer_sizes': (200, 100, 50), 'learning_rate': 'adaptive', 'learning_rate_init': 0.001, 'max_iter': 1000, 'n_iter_no_change': 20, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1}\n",
      "✅ Validation R²: 0.9785 | MAE: 0.0546 | RMSE: 0.0962\n",
      "Model stopped after 227 iterations.\n",
      "✅ Best mlp model saved to: ../model/10_features_mlp_gridsearch.joblib\n"
     ]
    }
   ],
   "source": [
    "param_grid_mlp = {\n",
    "    'hidden_layer_sizes': [(100, 50), (200, 100), (200, 100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'max_iter': [500, 1000],\n",
    "    'early_stopping': [True],\n",
    "    'validation_fraction': [0.1],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'n_iter_no_change': [10, 20],\n",
    "    'solver': ['adam'],\n",
    "    'tol': [1e-4]\n",
    "}\n",
    "\n",
    "best_model_mlp, best_params_mlp, val_metrics_mlp = grid_search_and_report_single_subset(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_set_7,\n",
    "    param_grid_mlp,\n",
    "    model_class=MLPRegressor,\n",
    "    model_name='mlp',\n",
    "    multi_output=False \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking-barbell-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
