{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e882268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5ea975e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_data = joblib.load('../data/interim/05_X_new_features_scaled.joblib')\n",
    "y_data = joblib.load('../data/interim/06_y_combined.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "478b3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356224, 91)\n",
      "(356224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_data.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "60b0c2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 356224 entries, 0 to 356223\n",
      "Data columns (total 91 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   Time                   356224 non-null  float64\n",
      " 1   1_red                  356224 non-null  float64\n",
      " 2   1_blue                 356224 non-null  float64\n",
      " 3   1_yellow               356224 non-null  float64\n",
      " 4   2_red                  356224 non-null  float64\n",
      " 5   2_blue                 356224 non-null  float64\n",
      " 6   2_yellow               356224 non-null  float64\n",
      " 7   3_red                  356224 non-null  float64\n",
      " 8   3_blue                 356224 non-null  float64\n",
      " 9   3_yellow               356224 non-null  float64\n",
      " 10  4_red                  356224 non-null  float64\n",
      " 11  4_blue                 356224 non-null  float64\n",
      " 12  4_yellow               356224 non-null  float64\n",
      " 13  5_red                  356224 non-null  float64\n",
      " 14  5_blue                 356224 non-null  float64\n",
      " 15  5_yellow               356224 non-null  float64\n",
      " 16  6_red                  356224 non-null  float64\n",
      " 17  6_blue                 356224 non-null  float64\n",
      " 18  6_yellow               356224 non-null  float64\n",
      " 19  global_mean            356224 non-null  float64\n",
      " 20  global_std             356224 non-null  float64\n",
      " 21  global_sum             356224 non-null  float64\n",
      " 22  red_color_mean         356224 non-null  float64\n",
      " 23  red_color_std          356224 non-null  float64\n",
      " 24  blue_color_mean        356224 non-null  float64\n",
      " 25  blue_color_std         356224 non-null  float64\n",
      " 26  yellow_color_mean      356224 non-null  float64\n",
      " 27  yellow_color_std       356224 non-null  float64\n",
      " 28  1_red_rolling_mean     356224 non-null  float64\n",
      " 29  1_red_rolling_std      356224 non-null  float64\n",
      " 30  1_blue_rolling_mean    356224 non-null  float64\n",
      " 31  1_blue_rolling_std     356224 non-null  float64\n",
      " 32  1_yellow_rolling_mean  356224 non-null  float64\n",
      " 33  1_yellow_rolling_std   356224 non-null  float64\n",
      " 34  2_red_rolling_mean     356224 non-null  float64\n",
      " 35  2_red_rolling_std      356224 non-null  float64\n",
      " 36  2_blue_rolling_mean    356224 non-null  float64\n",
      " 37  2_blue_rolling_std     356224 non-null  float64\n",
      " 38  2_yellow_rolling_mean  356224 non-null  float64\n",
      " 39  2_yellow_rolling_std   356224 non-null  float64\n",
      " 40  3_red_rolling_mean     356224 non-null  float64\n",
      " 41  3_red_rolling_std      356224 non-null  float64\n",
      " 42  3_blue_rolling_mean    356224 non-null  float64\n",
      " 43  3_blue_rolling_std     356224 non-null  float64\n",
      " 44  3_yellow_rolling_mean  356224 non-null  float64\n",
      " 45  3_yellow_rolling_std   356224 non-null  float64\n",
      " 46  4_red_rolling_mean     356224 non-null  float64\n",
      " 47  4_red_rolling_std      356224 non-null  float64\n",
      " 48  4_blue_rolling_mean    356224 non-null  float64\n",
      " 49  4_blue_rolling_std     356224 non-null  float64\n",
      " 50  4_yellow_rolling_mean  356224 non-null  float64\n",
      " 51  4_yellow_rolling_std   356224 non-null  float64\n",
      " 52  5_red_rolling_mean     356224 non-null  float64\n",
      " 53  5_red_rolling_std      356224 non-null  float64\n",
      " 54  5_blue_rolling_mean    356224 non-null  float64\n",
      " 55  5_blue_rolling_std     356224 non-null  float64\n",
      " 56  5_yellow_rolling_mean  356224 non-null  float64\n",
      " 57  5_yellow_rolling_std   356224 non-null  float64\n",
      " 58  6_red_rolling_mean     356224 non-null  float64\n",
      " 59  6_red_rolling_std      356224 non-null  float64\n",
      " 60  6_blue_rolling_mean    356224 non-null  float64\n",
      " 61  6_blue_rolling_std     356224 non-null  float64\n",
      " 62  6_yellow_rolling_mean  356224 non-null  float64\n",
      " 63  6_yellow_rolling_std   356224 non-null  float64\n",
      " 64  1_red_diff             356224 non-null  float64\n",
      " 65  1_blue_diff            356224 non-null  float64\n",
      " 66  1_yellow_diff          356224 non-null  float64\n",
      " 67  2_red_diff             356224 non-null  float64\n",
      " 68  2_blue_diff            356224 non-null  float64\n",
      " 69  2_yellow_diff          356224 non-null  float64\n",
      " 70  3_red_diff             356224 non-null  float64\n",
      " 71  3_blue_diff            356224 non-null  float64\n",
      " 72  3_yellow_diff          356224 non-null  float64\n",
      " 73  4_red_diff             356224 non-null  float64\n",
      " 74  4_blue_diff            356224 non-null  float64\n",
      " 75  4_yellow_diff          356224 non-null  float64\n",
      " 76  5_red_diff             356224 non-null  float64\n",
      " 77  5_blue_diff            356224 non-null  float64\n",
      " 78  5_yellow_diff          356224 non-null  float64\n",
      " 79  6_red_diff             356224 non-null  float64\n",
      " 80  6_blue_diff            356224 non-null  float64\n",
      " 81  6_yellow_diff          356224 non-null  float64\n",
      " 82  block_1_3_mean         356224 non-null  float64\n",
      " 83  block_4_6_mean         356224 non-null  float64\n",
      " 84  block_1_3_std          356224 non-null  float64\n",
      " 85  block_4_6_std          356224 non-null  float64\n",
      " 86  PCA_1                  356224 non-null  float64\n",
      " 87  PCA_2                  356224 non-null  float64\n",
      " 88  PCA_3                  356224 non-null  float64\n",
      " 89  PCA_4                  356224 non-null  float64\n",
      " 90  PCA_5                  356224 non-null  float64\n",
      "dtypes: float64(91)\n",
      "memory usage: 247.3 MB\n"
     ]
    }
   ],
   "source": [
    "X_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c7297cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(X_data, threshold=0.9, verbose=True):\n",
    "    \"\"\"\n",
    "    Returns names of features with reduced correlation (|corr| <= threshold).\n",
    "\n",
    "    Parameters:\n",
    "    - X_data (pd.DataFrame): The input DataFrame.\n",
    "    - threshold (float): Correlation threshold to consider for removal.\n",
    "    - verbose (bool): If True, prints the pairs and dropped columns.\n",
    "\n",
    "    Returns:\n",
    "    - remaining_features (list): List of remaining feature names.\n",
    "    - dropped_features (list): List of dropped feature names.\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = X_data.corr().abs()\n",
    "\n",
    "    # Mask the diagonal and lower triangle\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Find highly correlated pairs\n",
    "    high_corr_pairs = [\n",
    "        (col1, col2, corr_value)\n",
    "        for col1 in upper.columns\n",
    "        for col2, corr_value in upper[col1].items()\n",
    "        if corr_value > threshold\n",
    "    ]\n",
    "\n",
    "    # Optionally print the pairs\n",
    "    if verbose:\n",
    "        if high_corr_pairs:\n",
    "            print(\"Highly correlated feature pairs:\")\n",
    "            for col1, col2, corr_value in high_corr_pairs:\n",
    "                print(f\"{col1} and {col2} : {corr_value:.2f}\")\n",
    "        else:\n",
    "            print(\"No feature pairs exceed the correlation threshold.\")\n",
    "\n",
    "    # Identify columns to drop (second column of each pair)\n",
    "    to_drop = {col2 for col1, col2, corr_value in high_corr_pairs}\n",
    "\n",
    "    # Get remaining features\n",
    "    remaining_features = [col for col in X_data.columns if col not in to_drop]\n",
    "\n",
    "    # Optionally print the dropped and remaining columns\n",
    "    if verbose:\n",
    "        print(f\"Dropped columns: {list(to_drop)}\")\n",
    "        print(f\"Remaining columns: {len(remaining_features)} features\")\n",
    "\n",
    "    return remaining_features, list(to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95c8c61",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "29b9cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Features: 19\n",
      "PCA Features: 5\n",
      "Color Features: 6\n",
      "Rolling Features: 36\n",
      "Temporal Features: 18\n",
      "Sensor Features: 4\n"
     ]
    }
   ],
   "source": [
    "original_features = X_data.iloc[:, 0:19].columns.tolist()\n",
    "pca_features = [i for i in X_data.columns if \"PCA\" in i]\n",
    "color_features = [i for i in X_data.columns if \"color\" in i]\n",
    "rolling_features = [i for i in X_data.columns if \"rolling\" in i]\n",
    "temporal_features = [i for i in X_data.columns if \"diff\" in i]\n",
    "sensor_features = [i for i in X_data.columns if \"block\" in i]\n",
    "\n",
    "print(\"Original Features:\", len(original_features))\n",
    "print(\"PCA Features:\", len(pca_features))\n",
    "print(\"Color Features:\", len(color_features))\n",
    "print(\"Rolling Features:\", len(rolling_features))\n",
    "print(\"Temporal Features:\", len(temporal_features))\n",
    "print(\"Sensor Features:\", len(sensor_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a22d11c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_1 = original_features\n",
    "feature_set_2 = pca_features\n",
    "feature_set_3 = original_features + rolling_features + temporal_features \n",
    "feature_set_4 = original_features + color_features + sensor_features\n",
    "feature_set_5 = original_features + pca_features + color_features + rolling_features + temporal_features + sensor_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5b23fc",
   "metadata": {},
   "source": [
    "### Remove features that are highly corr with one another "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "16fa3cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_6, dropped_columns = remove_highly_correlated_features(X_data, threshold=0.8, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed89daa",
   "metadata": {},
   "source": [
    "### Feature Importance Ranking + Top-K Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ef64dfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_features_with_importance(X_data, y_data, top_k=10, model=None):\n",
    "    \"\"\"\n",
    "    Train a RandomForestRegressor and select top-k important features.\n",
    "    \n",
    "    Parameters:\n",
    "    - X_data (pd.DataFrame): Feature matrix\n",
    "    - y_data (pd.Series or pd.DataFrame): Target values\n",
    "    - top_k (int): Number of top features to select\n",
    "    - model (sklearn model, optional): Provide a pre-configured model if desired\n",
    "    \n",
    "    Returns:\n",
    "    - top_features (list): List of top-k feature names\n",
    "    - feature_importance_ranking (pd.DataFrame): Full ranking of features and importances\n",
    "    \"\"\"\n",
    "    if model is None:\n",
    "        model = RandomForestRegressor()\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_data, y_data)\n",
    "\n",
    "    # Get importances\n",
    "    importances = model.feature_importances_\n",
    "    feature_ranks = sorted(zip(X_data.columns, importances), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Prepare outputs\n",
    "    top_features = [f[0] for f in feature_ranks[:top_k]]\n",
    "    feature_importance_ranking = pd.DataFrame(feature_ranks, columns=['Feature', 'Importance'])\n",
    "\n",
    "    return top_features, feature_importance_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set_7, feature_importance_ranking = select_top_k_features_with_importance(X_data, y_data, top_k=10)\n",
    "\n",
    "print(\"Top 10 Features:\", feature_set_7)\n",
    "print(feature_importance_ranking.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff56a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of feature_set_0:  19\n",
      "length of feature_set_1:  5\n",
      "length of feature_set_2:  73\n",
      "length of feature_set_3:  29\n",
      "length of feature_set_4:  88\n",
      "length of feature_set_5:  33\n"
     ]
    }
   ],
   "source": [
    "feature_subset_list = [\n",
    "    feature_set_1, \n",
    "    feature_set_2, \n",
    "    feature_set_3, \n",
    "    feature_set_4, \n",
    "    feature_set_5, \n",
    "    feature_set_6, \n",
    "    feature_set_7\n",
    "]\n",
    "\n",
    "for i, subset in enumerate(feature_subset_list):\n",
    "    print(f\"length of feature_set_{i}: \", len(subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9df0b0",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6dfc87ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation (80/20 split)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "098fe964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets,\n",
    "    model_class,\n",
    "    model_name,\n",
    "    model_save_dir='../model/',\n",
    "    multi_output=False,\n",
    "    **model_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Generic function to evaluate multiple feature subsets with optional multi-output support,\n",
    "    save the best model, and report performance.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train, y_train, X_val, y_val: Pre-split data\n",
    "    - feature_subsets (list): List of feature name lists\n",
    "    - model_class: Scikit-learn model class (e.g., RandomForestRegressor)\n",
    "    - model_name (str): Name to tag the saved model file\n",
    "    - model_save_dir (str): Directory to save model\n",
    "    - multi_output (bool): Whether to wrap with MultiOutputRegressor\n",
    "    - model_kwargs: Additional model parameters\n",
    "\n",
    "    Returns:\n",
    "    - best_model: Trained best model\n",
    "    - best_metrics (dict): R2, MAE, RMSE\n",
    "    - best_features (list): Features used\n",
    "    \"\"\"\n",
    "    best_r2 = -np.inf\n",
    "    best_model = None\n",
    "    best_metrics = {}\n",
    "    best_features = []\n",
    "\n",
    "    for idx, feature_subset in enumerate(feature_subsets, start=1):\n",
    "        print(f\"\\nEvaluating Feature Set {idx} with {len(feature_subset)} features using {model_name}...\")\n",
    "\n",
    "        X_train_subset = X_train[feature_subset]\n",
    "        X_val_subset = X_val[feature_subset]\n",
    "\n",
    "        # Build model with optional multi-output wrapper\n",
    "        base_model = model_class(**model_kwargs)\n",
    "        model = MultiOutputRegressor(base_model) if multi_output else base_model\n",
    "        \n",
    "        print(f\"Full model parameters:\\n{base_model.get_params()}\\n\")\n",
    "\n",
    "        # Train and predict\n",
    "        model.fit(X_train_subset, y_train)\n",
    "        y_pred = model.predict(X_val_subset)\n",
    "\n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_val, y_pred)\n",
    "        mae = mean_absolute_error(y_val, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "\n",
    "        print(f\"R² Score: {r2:.4f} | MAE: {mae:.4f} | RMSE: {rmse:.4f}\")\n",
    "\n",
    "        # Update best model if better\n",
    "        if r2 > best_r2:\n",
    "            best_r2 = r2\n",
    "            best_model = model\n",
    "            best_metrics = {'R2': r2, 'MAE': mae, 'RMSE': rmse}\n",
    "            best_features = feature_subset\n",
    "\n",
    "    # Save best model\n",
    "    os.makedirs(model_save_dir, exist_ok=True)\n",
    "    model_path = os.path.join(model_save_dir, f'best_{model_name}.joblib')\n",
    "    joblib.dump(best_model, model_path)\n",
    "    print(f\"\\n✅ Best {model_name} model saved to: {model_path}\")\n",
    "\n",
    "    return best_model, best_metrics, best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce4cde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model, rf_metrics, rf_features = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=RandomForestRegressor,\n",
    "    model_name='random_forest',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "gb_model, gb_metrics, gb_features = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=GradientBoostingRegressor,\n",
    "    model_name='gradient_boosting',\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4378f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support Vector Regressor\n",
    "svr_model, svr_metrics, svr_features = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=SVR,\n",
    "    model_name='svr',\n",
    "    kernel='rbf', C=1.0, epsilon=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP\n",
    "mlp_model, mlp_metrics, mlp_features = evaluate_and_save_best_model_generic(\n",
    "    X_train, y_train, X_val, y_val,\n",
    "    feature_subsets=feature_subset_list,\n",
    "    model_class=MLPRegressor,\n",
    "    model_name='mlp',\n",
    "    random_state=42,\n",
    "    max_iter=1000,                # Important for convergence\n",
    "    hidden_layer_sizes=(100, 50), # Example architecture: 2 layers, 100 and 50 neurons\n",
    "    activation='relu',            # Activation function\n",
    "    solver='adam',                # Optimizer\n",
    "    learning_rate='adaptive',     # Learning rate strategy\n",
    "    early_stopping=True           # Optional: stops if validation score stops improving\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking-barbell-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
